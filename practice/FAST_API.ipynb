{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Práctica parte FastAPI"
      ],
      "metadata": {
        "id": "P061X_l_jeHF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnBp0ACVjY0e",
        "outputId": "e6c6309e-ac32-4c3c-ea2f-a8fff32089cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/94.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m92.2/94.8 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/71.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q fastapi uvicorn pyngrok scikit-learn transformers joblib nest_asyncio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crear API"
      ],
      "metadata": {
        "id": "3SHhnz89mUVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Configurar ngrok"
      ],
      "metadata": {
        "id": "-XRIr_FLpvh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2tN08g6yTDm1kjgafyqZfk5BBEp_6fNE8rfjq9sFQKnJM8M9R"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnITybuDp-la",
        "outputId": "2286e4b1-b94b-4c80-9d98-3346ab17f4a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from transformers import pipeline\n",
        "from pyngrok import ngrok\n",
        "from typing import Dict\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "import threading\n",
        "\n",
        "# Aplicar nest_asyncio para evitar el error de asyncio.run()\n",
        "nest_asyncio.apply()\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Base de datos en memoria\n",
        "models_db  = {\n",
        "    \"sentiment-analysis\": {\"description\": \"Modelo para analizar sentimientos\"},\n",
        "    \"summarization\": {\"description\": \"Modelo para resumir textos\"},\n",
        "}\n",
        "\n",
        "# Generación de texto\n",
        "@app.post(\"/generate_text\")\n",
        "def generate_text(prompt: str):\n",
        "    text_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "    generated_text = text_generator(prompt, max_length=50, num_return_sequences=1)\n",
        "    return {\"generated_text\": generated_text[0][\"generated_text\"]}\n",
        "\n",
        "# Obtener lista de modelos disponibles\n",
        "@app.get(\"/models\")\n",
        "def get_models():\n",
        "    \"\"\"Obtiene la lista de modelos registrados en la API.\"\"\"\n",
        "    return {\"models\": models_db}\n",
        "\n",
        "# Agregar un nuevo modelo\n",
        "class ModelCreateRequest(BaseModel):\n",
        "    description: str\n",
        "\n",
        "@app.put(\"/models/{model_name}\")\n",
        "def add_model(model_name: str, model_data: ModelCreateRequest):\n",
        "    if model_name in models_db:\n",
        "        raise HTTPException(status_code=400, detail=\"El modelo ya existe\")\n",
        "\n",
        "    models_db[model_name] = {\"description\": model_data.description}\n",
        "    return {\"message\": f\"Modelo '{model_name}' agregado correctamente\", \"models\": models_db}\n",
        "\n",
        "# Eliminar un modelo\n",
        "@app.delete(\"/models/{model_name}\")\n",
        "def delete_model(model_name: str):\n",
        "    if model_name not in models_db:\n",
        "        raise HTTPException(status_code=404, detail=\"El modelo no existe\")\n",
        "\n",
        "    del models_db[model_name]\n",
        "    return {\"message\": f\"Modelo '{model_name}' eliminado correctamente\", \"models\": models_db}\n",
        "\n",
        "# Endpoint de resumen de texto usando Transformers\n",
        "@app.get('/summary')\n",
        "def summary_generator(query: str):\n",
        "    summarization_pipeline = pipeline('summarization')\n",
        "    return summarization_pipeline(query)\n",
        "\n",
        "# Iniciar el servidor en un puerto específico\n",
        "def start_api():\n",
        "    # Abrir el túnel ngrok\n",
        "    public_url = ngrok.connect(8000).public_url\n",
        "    print(f\"API disponible en: {public_url}\")\n",
        "\n",
        "    # Ejecutar Uvicorn en un hilo\n",
        "    server_thread = threading.Thread(target=uvicorn.run, args=(app,), kwargs={\"host\": \"0.0.0.0\", \"port\": 8000})\n",
        "    server_thread.start()"
      ],
      "metadata": {
        "id": "eq-G1247mRZH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_api()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nm8U0q5tmjjX",
        "outputId": "8accf8a8-93b1-4523-bf53-132f359fe6bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API disponible en: https://a43b-34-171-247-22.ngrok-free.app\n"
          ]
        }
      ]
    }
  ]
}